[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "WaitWhat",
    "section": "",
    "text": "Ratetables\n\n\n\n\n\n\n\nGLMs\n\n\nInsurance\n\n\nPricing\n\n\n\n\nWhere we build rate tables\n\n\n\n\n\n\nDec 16, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/ratetables/Ratetables.html",
    "href": "posts/ratetables/Ratetables.html",
    "title": "Ratetables",
    "section": "",
    "text": "“The motor insurance industry is big. Really big. You just won’t believe how vastly hugely mind-bogglingly big it is. So why is it still using Ratetables for pricing?” - Not Douglas Adams\nFake Douglas Adams is right. Despite the oceans of premium we collect every year and whatever irresponsible bigwigs like to tell investors about insurers’ use of analytics, we still use ratetables to do most of our pricing. Do not be fooled by our teams of data scientists and espresso-machined-open-plan offices, when it comes down to big-money items like premiums you will have to pry ratetables from our cold dead hands.\nWhen an insurance exec says something like “Our innovative AI-powered solution leverages advanced machine learning algorithms to revolutionize the way we [something something insurance]” what he’s saying is “We have a chatbot”. That’s not to say we don’t use machine learning behind the scenes! Despite what insurtechs like to tell VCs about stuffy tradsurers we aren’t actually Luddites who havent evolved beyond dusty scrolls and tea-leaves. But we are exceptionally opinionated and when it comes to tabular modelling, its depressingly difficult to out-perform a well-made ratetable. We know - we’ve tried.\nBut what IS a ratetable? It barely gets any mention when searched online and despite it being used to transact a gazillion monies of premium per year, there really isn’t much material showing you what they are or how to build one. I think this is shame because the flexibility, transparency and ease of implementation should make them the default way to deploy linear-models.\nThe tldr version, a ratetable is insurance-speak for a collection of VLOOKUPs. They are basically GLMs converted to tables with lookup values per variable level….And thats about it. Its so embarrassingly simple we are too embarrassed to talk about it. If you’ve ever used VLOOKUP in Excel, congratulations you’ve used a ratetable.\nSlightly wordier version, ratetables are GLMs that have been converted to a series of mutliplicative lookup tables with each variable in the model rescaled to some reference level providing users more intuitive understanding of the effect each variable has on the target and the intercept of the model adjusted by an offsetting amount to ensure the predictions still tally up."
  },
  {
    "objectID": "posts/ratetables/Ratetables.html#building-a-ratetable",
    "href": "posts/ratetables/Ratetables.html#building-a-ratetable",
    "title": "Ratetables",
    "section": "Building a ratetable",
    "text": "Building a ratetable\n\nIf you’re a Data Scientist you’d probably call this ‘Individual Conditional Expectations’ (ICE )\nIf you’re the rubber-ducky on my desk you know this as ’Take a random line of data, duplicate it a bunch of times and change the values of just the variable you care about and see how the predictions changed\n\nIts quite rate that you’ll have a nice and simple GLM of the form I used above with ‘clean’ coefficients. Its much more likely you’ll have splines or polynomials used on some of the numeric variables, so the direct approach of converting the coefficients into tables wont work. Instead this method works for any GLM/GAM provided there aren’t any interactions (the second method works better for this).\nWe are going to use the insuraceData package, load up the AutoClaims dataset and then build a simple GLM for claim severity. We don’t care that the model itself I just want to focus on the actual ratetabley bits.\n\nlibrary(tidyverse)\nlibrary(insuranceData)\nlibrary(data.table)\nlibrary(mgcv) # fit gam\n\ndata(AutoClaims)\n\n\n\n# fit numeric variables as GAMs to avoid having to\nclaims_glm &lt;- mgcv::gam(PAID ~\n                    s(pmin(AGE,95),k=3, bs=\"cr\") +\n                    GENDER + \n                    CLASS +\n                    STATE,\n                  data = AutoClaims,\n                  family = Gamma(\"log\"))\n\n\n# model output\nsummary(claims_glm)\n\n\nFamily: Gamma \nLink function: log \n\nFormula:\nPAID ~ s(pmin(AGE, 95), k = 3, bs = \"cr\") + GENDER + CLASS + \n    STATE\n\nParametric coefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    7.372967   0.121985  60.441  &lt; 2e-16 ***\nGENDERM       -0.008085   0.035549  -0.227  0.82009    \nCLASSC11      -0.001279   0.068666  -0.019  0.98513    \nCLASSC1A      -0.052953   0.169438  -0.313  0.75465    \nCLASSC1B       0.032155   0.087917   0.366  0.71457    \nCLASSC1C      -0.214266   0.235316  -0.911  0.36257    \nCLASSC2       -0.318220   0.188683  -1.687  0.09174 .  \nCLASSC6       -0.018006   0.078295  -0.230  0.81811    \nCLASSC7        0.002340   0.072101   0.032  0.97411    \nCLASSC71       0.016351   0.070098   0.233  0.81557    \nCLASSC72       0.225273   0.163104   1.381  0.16728    \nCLASSC7A       0.055853   0.143855   0.388  0.69783    \nCLASSC7B       0.166986   0.078150   2.137  0.03265 *  \nCLASSC7C       0.195217   0.166980   1.169  0.24240    \nCLASSF1       -0.078495   0.267407  -0.294  0.76912    \nCLASSF11       0.116224   0.230421   0.504  0.61400    \nCLASSF6       -0.006399   0.130963  -0.049  0.96103    \nCLASSF7       -0.495043   0.192008  -2.578  0.00995 ** \nCLASSF71      -0.059815   0.157060  -0.381  0.70333    \nSTATESTATE 02  0.108242   0.117960   0.918  0.35885    \nSTATESTATE 03  0.118306   0.133515   0.886  0.37560    \nSTATESTATE 04  0.059744   0.123815   0.483  0.62945    \nSTATESTATE 06  0.263534   0.124330   2.120  0.03407 *  \nSTATESTATE 07  0.181269   0.140526   1.290  0.19712    \nSTATESTATE 10  0.157797   0.139320   1.133  0.25741    \nSTATESTATE 11  0.097603   0.483004   0.202  0.83986    \nSTATESTATE 12  0.393129   0.143128   2.747  0.00604 ** \nSTATESTATE 13  0.225654   0.147946   1.525  0.12725    \nSTATESTATE 14  0.050829   0.154268   0.329  0.74180    \nSTATESTATE 15  0.083418   0.114447   0.729  0.46610    \nSTATESTATE 17  0.210776   0.128050   1.646  0.09980 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                   edf Ref.df     F p-value  \ns(pmin(AGE, 95)) 1.923  1.994 3.757  0.0197 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.0036   Deviance explained = 1.46%\nGCV = 1.1323  Scale est. = 1.9841    n = 6773\n\n\nNow we build a function that can take in the model, the dataset, the variable we want to build the relativity table for and a set of values we want to test it on using the rubber-ducky approach\n\nmake_rate_table &lt;- function(model,dataset,variable, variable_values){\n \n  dataset &lt;- setDT(dataset) # convert to data.table\n  base_prediction &lt;- dataset[sample(.N, 1)] #sample a single line from the dataset\n  col_nr &lt;- which(names(dataset)==variable) # find the column number for the variable\n  base_prediction$rep &lt;- length(variable_values) #new column that tells us how many lines of data we need to duplicate our sample\n \n  expd &lt;- splitstackshape::expandRows(base_prediction, \"rep\")\n  expd[,col_nr] &lt;- variable_values # replace the variable with the values provided\n \n  # predictions &lt;- predict.glm(model, newdata = expd, type= \"response\")\n  predictions &lt;- mgcv::predict.gam(model, newdata = expd, type= \"response\")\n \n  # scale the predictions relative to the lowest value so we get relativities\n  return(data.frame(Level = variable_values,\n                    Relativity = predictions/min(predictions)))\n \n \n}\n\nWe can apply this function to create the rate table for the Driver Age variable\n\n# create the table of relativities\nage_relativities &lt;- make_rate_table(claims_glm,\n                dataset = AutoClaims,\n                variable = \"AGE\",\n                variable_values = sort(unique(AutoClaims$AGE)))\n\n# lets look at the table\nknitr::kable(age_relativities[1:10,])\n\n\n\n\nLevel\nRelativity\n\n\n\n\n50\n1.059501\n\n\n51\n1.053348\n\n\n52\n1.047287\n\n\n53\n1.041370\n\n\n54\n1.035650\n\n\n55\n1.030180\n\n\n56\n1.025009\n\n\n57\n1.020187\n\n\n58\n1.015762\n\n\n59\n1.011785\n\n\n\n\n# plot these\nggplot(age_relativities,aes(x=Level,y=Relativity))+\n  geom_line()+\n  geom_point()+\n  ggtitle(\"Driver Age Relativities\",\n          subtitle = \"Very weird shape!\")+\n  theme_minimal()\n\n\n\n\nWe’ve created our first table for the driver age. We now need to do this for all the other variables using the same idea.\n\ntheme_set(theme_minimal())\n# List of variables\nvariables = c(\"GENDER\", \"CLASS\", \"STATE\")\n\n# Initialize an empty list to store the relativity tables\nrelativities = list()\n\n# Loop to create relativity tables\nfor (var in variables) {\n  relativities[[var]] &lt;- make_rate_table(claims_glm,\n                                         dataset = AutoClaims,\n                                         variable = var,\n                                         variable_values = sort(unique(AutoClaims[[var]])))\n \n\n}\n\n\n# Initialize an empty list for plots\nplots = list()\n\n# Loop to create ggplot objects for each relativity\nfor (var in names(relativities)) {\n  plots[[var]] &lt;- ggplot(relativities[[var]], aes(x=Level, y=Relativity)) +\n                    geom_col() +\n                    theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=1)) + \n                    ggtitle(paste(var, \"Relativities\"))\n}\n\n# Loop to display plots\nfor (plot_name in names(plots)) {\n  print(plots[[plot_name]])\n}\n\n\n\n\n\n\n\n\n\n# Plotting (example for one plot, you can loop or selectively plot as needed)\n\n\nRebasing\nOk now you’ll see the first downside to this approach is we need to adjust the intercept of the model to make the predictions tally up to the original GLM. To make predictions with this model we will left-join the original dataset to the relativities and calculate the new intercept required to make the predictions line up.\n\n# we need to join the main dataset to the relativity tables\nTally &lt;- AutoClaims |&gt;\n  left_join(relativities[['GENDER']] |&gt;\n              select(Level, Relativity_1 = Relativity),\n            by = c(\"GENDER\" = \"Level\")) |&gt;\n  left_join(relativities[['CLASS']] |&gt; select(Level, Relativity_2 = Relativity),\n            by = c(\"CLASS\" = \"Level\")) |&gt;\n  left_join(relativities[['STATE']] |&gt; select(Level, Relativity_3 = Relativity),\n            by = c(\"STATE\" = \"Level\")) |&gt;\n  left_join(age_relativities |&gt; select(Level, Relativity_4 = Relativity),\n            by = c(\"AGE\" = \"Level\")) |&gt;\n  mutate(AllRelativities = matrixStats::rowProds(as.matrix(across(\n    matches(\"Relativity_[0-9]\")\n  )))) |&gt;\n  mutate(BaseRate = mean(AutoClaims$PAID)) |&gt; # set the baserate = avg claim size as starting point\n  mutate(RateTablePredictions = AllRelativities * BaseRate) |&gt;\n  mutate(GLM_Prediction = predict(claims_glm, AutoClaims, type = \"response\"))\n\n\nhead(Tally)\n\n      STATE CLASS GENDER AGE    PAID Relativity_1 Relativity_2 Relativity_3\n1: STATE 14   C6       M  97 1134.44     1.000000     1.611293     1.052143\n2: STATE 15   C6       M  96 3761.24     1.000000     1.611293     1.086996\n3: STATE 15   C11      M  95 7842.31     1.000000     1.638471     1.086996\n4: STATE 15   F6       F  95 2384.67     1.008118     1.630104     1.086996\n5: STATE 15   F6       M  95  650.00     1.000000     1.630104     1.086996\n6: STATE 15   F6       M  95  391.12     1.000000     1.630104     1.086996\n   Relativity_4 AllRelativities BaseRate RateTablePredictions GLM_Prediction\n1:      1.48686        2.520691 1853.035             4670.927       2322.296\n2:      1.48686        2.604190 1853.035             4825.654       2399.223\n3:      1.48686        2.648116 1853.035             4907.050       2439.691\n4:      1.48686        2.655980 1853.035             4921.622       2446.937\n5:      1.48686        2.634592 1853.035             4881.990       2427.232\n6:      1.48686        2.634592 1853.035             4881.990       2427.232\n\n\nThankfully its quite easy to this as all we need to do is adjust the intercept/baserate of the model such that it still gives the same average response as the original GLM. Lets show what this looks like when plotting the actual vs expected values for the AGE variable.\n\n# lets plot the predictions against the GLM versus the AGE variable\n Tally |&gt;\n   group_by(AGE) |&gt;\n   summarise(\n     Actual = mean(PAID),\n            GLM = mean(GLM_Prediction),\n            RateTable = mean(RateTablePredictions),\n            RateTable_Rebased = mean(RateTablePredictions*(mean(Tally$GLM)/(mean(Tally$RateTablePredictions))))) |&gt;\n   ggplot()+\n   geom_line(aes(x=AGE,y=Actual,group=1,colour=\"Actual\"))+\n   geom_line(aes(x=AGE,y=GLM,group=1,colour=\"GLM\"))+\n   geom_line(aes(x=AGE,y=RateTable,group=1,colour=\"RateTable\"))+\n   geom_point(aes(x=AGE,y=RateTable_Rebased,group=1,colour=\"RateTable_Rebased\"),size=2)+\n   ggtitle(\"Actual vs Predicted Comparison for AGE\", subtitle = \"Rebasing makes it align perfectly with the original GLM\")\n\n\n\n\nAs you can see the predictions from the ratetables line up exactly with the original model once we rebase the predictions. However while rebasing is not difficult it is tedious, especially when you have dozens of variables to work through.\n\n\nInteractions\nThe second thing to be aware of with this approach is it can give misleading relativities when you’re trying to isolate the effect of interactions. This is because ICE will vary all aspects of the model that contain the variable of interest including the interactions, so for example say we had AGE interacted with STATE, when varying the AGE variable to create its ratetable, we will also be picking up the effect of AGE:STATE. This is a pain to work reverse out and my suggestion when using ICE is to make duplicates for all variables you want to interact in order to isolate their effects.\nAgain this is best shown with an example. Lets interact AGE and STATE and illustrate the differences in relativities caused by interactions when blindly applying the ICE approach:\n\nlibrary(cowplot)\n\n# new model interacting AGE & CLASS\nclaims_glm_interactions &lt;- mgcv::gam(PAID ~\n                    s(pmin(AGE,95),k=3, bs=\"cr\") +\n                    CLASS +\n                    STATE +\n                    GENDER + \n                    AGE:STATE,\n                  data = AutoClaims,\n                  family = Gamma(\"log\"))\n\n# duplicate AGE and CLASS and build new GLM off the duplicated columns\nAutoClaims$AGE_2 &lt;- AutoClaims$AGE\nAutoClaims$STATE_2 &lt;- AutoClaims$STATE\n\n\nclaims_glm_interactions2 &lt;- mgcv::gam(PAID ~\n                    s(pmin(AGE,95),k=3, bs=\"cr\") +\n                    CLASS +\n                    STATE +\n                    GENDER + \n                    AGE_2:STATE_2,\n                  data = AutoClaims,\n                  family = Gamma(\"log\"))\n\n\n# ICE function for two variables\nmake_rate_table_2var &lt;- function(model,\n                                 dataset,\n                                 variable1,\n                                 variable2, \n                                 variable1_values,\n                                 variable2_values\n                                 ){\n \n  dataset &lt;- setDT(dataset) # convert to data.table\n  base_prediction &lt;- dataset[sample(.N, 1)] #sample a single line from the dataset\n  \n  baseline &lt;- as.numeric(mgcv::predict.gam(model, newdata = base_prediction, type= \"response\"))\n  \n  \n  col_nr1 &lt;- which(names(dataset)==variable1) # find the column number for the 1st variable\n  col_nr2 &lt;- which(names(dataset)==variable2) # find the column number for the 2nd variable\n  \n  # create every unique combination of the var1 and var2\n  uniq_comb &lt;- expand.grid(variable1_values,variable2_values)\n  colnames(uniq_comb) &lt;- c(variable1,variable2)\n  \n  base_prediction$rep &lt;- nrow(uniq_comb) #new column that tells us how many lines of data we need to duplicate our sample\n \n  expd &lt;- splitstackshape::expandRows(base_prediction, \"rep\")\n  expd[,col_nr1] &lt;- uniq_comb[,1] # replace the variable with the values provided\n  expd[,col_nr2] &lt;- uniq_comb[,2] # replace the variable with the values provided\n \n  # predictions &lt;- predict.glm(model, newdata = expd, type= \"response\")\n  predictions &lt;- mgcv::predict.gam(model, newdata = expd, type= \"response\")\n \n  expd &lt;- data.frame(expd)\n  \n  final_frame = data.frame(expd[,col_nr1],\n                    expd[,col_nr2],\n                    Relativity = predictions/(baseline))\n  \n  colnames(final_frame) &lt;- c(variable1,variable2,\"Relativity\")\n  \n  # scale the predictions relative to the lowest value so we get relativities\n  return(final_frame)\n \n \n}\n \n\n\np1 &lt;- make_rate_table_2var(claims_glm_interactions,\n                                 AutoClaims,\n                                 \"AGE\",\n                                 \"STATE\", \n                                variable1_values =  sort(unique(AutoClaims$AGE)),\n                                variable2_values =  sort(unique(AutoClaims$STATE))\n                                 ) %&gt;% \n  ggplot(aes(x=AGE,y=Relativity,group=STATE, colour=STATE))+\n  geom_line()+\n  ggtitle(\"AGE:STATE Incorrect\", subtitle = \"Lies, deception\")\n\np2 &lt;- make_rate_table_2var(claims_glm_interactions2,\n                                 AutoClaims,\n                                 \"AGE_2\",\n                                 \"STATE_2\", \n                                variable1_values =  sort(unique(AutoClaims$AGE_2)),\n                                variable2_values =  sort(unique(AutoClaims$STATE_2))\n                                 ) %&gt;% \n  ggplot(aes(x=AGE_2,y=Relativity,group=STATE_2, colour=STATE_2))+\n  geom_line()+\n  ggtitle(\"AGE:STATE Correct\", subtitle = \"The other thing\")\n\n\nplot_grid(p1, p2)\n\n\n\n\nAs you can see above, the two graphs show totally different relativities for what should be the same thing, however this is simply because in the first approach we are picking up the effect of the interactions and the individual effects of AGE and STATE. The second graph is the true interaction effect and is what would be used in your ratetable.\nThere is another method I use to build ratetables to try and get around the rebasing and interaction issues which I will write about in future. This second method is a lot more ‘fiddly’ and needs adjusting to different modelling packages but does involve the use of the word matrix which immediately gives us extra LinkedIn-points."
  }
]