{
  "hash": "81e3822e7dc39d107d0463252268e7e5",
  "result": {
    "markdown": "---\ntitle: 'Ratetables'\ndate: '2023-12-16'\n#author: \"Serban Dragne\"\ncategories: ['GLMs','Insurance','Pricing']\ndescription: 'Where we build rate tables'\nexecute:\n  message: false\n  warning: false\neditor_options:\n  chunk_output_type: console\n---\n\n\n> \"The motor insurance industry is big. Really big. You just won't believe how vastly hugely mind-bogglingly big it is. So why is it still using Ratetables for pricing?\" - Not Douglas Adams\n\nFake Douglas Adams is right. Despite the oceans of premium we collect every year and whatever irresponsible bigwigs like to tell investors about insurers' use of analytics, we still use ratetables to do most of our pricing. Do not be fooled by our teams of data scientists and espresso-machined-open-plan offices, when it comes down to big-money items like premiums you will have to pry ratetables from our cold dead hands.\n\nWhen an insurance exec says something like *\"Our innovative AI-powered solution leverages advanced machine learning algorithms to revolutionize the way we \\[something something insurance\\]\"* what he's saying is \"**We have a chatbot**\". That's not to say we don't use machine learning behind the scenes! Despite what insurtechs like to tell VCs about stuffy tradsurers we aren't actually Luddites who havent evolved beyond dusty scrolls and tea-leaves. But we are exceptionally opinionated and when it comes to tabular modelling, its depressingly difficult to out-perform a well-made ratetable. We know - we've tried.\n\nBut what **IS** a ratetable? It barely gets any mention when searched online and despite it being used to transact a gazillion monies of premium per year, there really isn't much material showing you what they are or how to build one. I think this is shame because the flexibility, transparency and ease of implementation should make them the default way to deploy linear-models.\n\nThe tldr version, a ratetable is insurance-speak for a collection of VLOOKUPs. They are basically GLMs converted to tables with lookup values per variable level....And thats about it. Its so embarrassingly simple we are too embarrassed to talk about it. If you've ever used VLOOKUP in Excel, congratulations you've used a ratetable.\n\nSlightly wordier version, ratetables are GLMs that have been converted to a series of mutliplicative lookup tables with each variable in the model rescaled to some reference level providing users more intuitive understanding of the effect each variable has on the target and the intercept of the model adjusted by an offsetting amount to ensure the predictions still tally up.\n\n### Examples\n\nImagine we have a GLM with the following formula $y=e^{(0.1x + 0.05)}$ and $x \\in [10, 20]$ how would we represent this as a ratetable? Well lets plug values into the formula\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nx <- seq(from=10,to=20,by=1)\ny <- sapply(x, \\(x) {exp(0.1*x)})\n\ntibble(\"x\" = x,\n       \"y=exp(0.1*x+0.05) \" = exp(x*0.1 + 0.05)\n       )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 11 × 2\n       x `y=exp(0.1*x+0.05) `\n   <dbl>                <dbl>\n 1    10                 2.86\n 2    11                 3.16\n 3    12                 3.49\n 4    13                 3.86\n 5    14                 4.26\n 6    15                 4.71\n 7    16                 5.21\n 8    17                 5.75\n 9    18                 6.36\n10    19                 7.03\n11    20                 7.77\n```\n:::\n:::\n\n\nAs you can see above there is a 1:1 relationship between $x$ and $y$ . Congratulations you just made a *almost*-ratetable.\n\nNow to make it into a ratetable we split out the calculations into the effect of $x$ and the intercept.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\"x\" = x,\n       \"y=exp(0.1*x+0.05) \" = exp(x*0.1 + 0.05),\n       \"intercept\" = exp(0.05),\n       \"effect of x\" = exp(x*0.1)\n       )  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 11 × 4\n       x `y=exp(0.1*x+0.05) ` intercept `effect of x`\n   <dbl>                <dbl>     <dbl>         <dbl>\n 1    10                 2.86      1.05          2.72\n 2    11                 3.16      1.05          3.00\n 3    12                 3.49      1.05          3.32\n 4    13                 3.86      1.05          3.67\n 5    14                 4.26      1.05          4.06\n 6    15                 4.71      1.05          4.48\n 7    16                 5.21      1.05          4.95\n 8    17                 5.75      1.05          5.47\n 9    18                 6.36      1.05          6.05\n10    19                 7.03      1.05          6.69\n11    20                 7.77      1.05          7.39\n```\n:::\n:::\n\n\nAnd right here we have a simplest ratetable with one variable and all we had to do was isolate the effect of the sole variable into a lookup table.\n\nUsually ratetables rescale the effect of each variable to be in relation to some reference level i.e. some value we use to say has no effect and we compare other values against this. We like to do this because it makes it more intuitive to discuss the effects of different variables. In our example we would say $x=10$ increases the response approx. 2.7 times and would give the misleading impression that somehow 10 is very 'risky' until you realise that every other value of x results in a higher response, so in effect rather than increasing the risk 2.7, relative to the 'average' value that x can take on, x=10 is actually less risky and would reduce the risk versus the average.\n\nLets make an adjustment. Say we know $x=13$ is the most common value of $x$ and choose this as the baseline, so lets divide the effect of variable $x$ by $f(x=13):3.857426$ to create the relativity table for x and adjust the intercept by an offsetting amount.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\"x\" = x,\n       \"y=exp(0.1*x+0.05) \" = exp(x*0.1 + 0.05),\n       \"New Intercept\" = exp(13*0.1 + 0.05),\n       \"Rescaled-x\" = exp(x*0.1)/(exp(13*0.1))\n       )  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 11 × 4\n       x `y=exp(0.1*x+0.05) ` `New Intercept` `Rescaled-x`\n   <dbl>                <dbl>           <dbl>        <dbl>\n 1    10                 2.86            3.86        0.741\n 2    11                 3.16            3.86        0.819\n 3    12                 3.49            3.86        0.905\n 4    13                 3.86            3.86        1    \n 5    14                 4.26            3.86        1.11 \n 6    15                 4.71            3.86        1.22 \n 7    16                 5.21            3.86        1.35 \n 8    17                 5.75            3.86        1.49 \n 9    18                 6.36            3.86        1.65 \n10    19                 7.03            3.86        1.82 \n11    20                 7.77            3.86        2.01 \n```\n:::\n:::\n\n\n### \n\nWhat if we have more than 1 variable? Its exactly the same idea. Lets modify the above formula, $y=\\exp^{(0.1x - 0.05y + 0.05)}$ with $y= \\in [0,5]$ this time choosing $y=3$ as the reference level for $y$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(from=10,to=20,by=1)\ny <- seq(from=0, to=5, by=1)\n\ntibble(\"x\" = x,\n       \"relativities for x\" = exp(x*0.1)/(exp(13*0.1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 11 × 2\n       x `relativities for x`\n   <dbl>                <dbl>\n 1    10                0.741\n 2    11                0.819\n 3    12                0.905\n 4    13                1    \n 5    14                1.11 \n 6    15                1.22 \n 7    16                1.35 \n 8    17                1.49 \n 9    18                1.65 \n10    19                1.82 \n11    20                2.01 \n```\n:::\n\n```{.r .cell-code}\ntibble(\"y\" = y,\n       \"relativities for y\" = exp(y*(-0.05))/(exp(3*(-0.05))))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n      y `relativities for y`\n  <dbl>                <dbl>\n1     0                1.16 \n2     1                1.11 \n3     2                1.05 \n4     3                1    \n5     4                0.951\n6     5                0.905\n```\n:::\n:::\n\n\nAs we rescaled both $x$ and $y$ we need to adjust the model's intercept, so in this case the new baseline would be $e^{13*0.1 + 3*(-0.05) + 0.05}$ in order for the model's predictions to still give the same predictions.\n\nI wont rant too much about why insurers still use ratetables instead of other tabular-style models or even 'pure' GLMS, but my shortlist contenders:\n\n-   Most off-the-shelf systems are built around deploying table-based models and it is a major IT job to migrate away from them.\n-   The interpretability and transparency provided by ratetables cant be matched.\n-   Ease of deployment. Docker? Pffft, try pen and paper. Want to make your model more complex? Use the other side of the page\n-   'Tweakability'. Do you want to force a certain behavior from your model? Ratetables make this trivial and you can treat them like Lego blocks. This is essential for when you're launching a new product and you're data-poor.\n\nThere are a few tools out there that cast the spells to automatically build and export GLMs as ratetables but all these tools are expensive and require talking to sales representatives.\n\nAnd since my wife is busy getting her hair done and we have about 2 hours to kill in this Starbucks we're going to make our own ratetables\n\n!['Cos I'm a mature professional](Bender.jpeg){fig-align=\"center\"}\n\n## Building a ratetable\n\n-   If you're a Data Scientist you'd probably call this '*Individual Conditional Expectations*' (***ICE*** )\n\n-   If you're the rubber-ducky on my desk you know this as '*Take a random line of data, duplicate it a bunch of times and change the values of just the variable you care about and see how the predictions changed*\n\nIts quite rate that you'll have a nice and simple GLM of the form I used above with 'clean' coefficients. Its much more likely you'll have splines or polynomials used on some of the numeric variables, so the direct approach of converting the coefficients into tables wont work. Instead this method works for any GLM/GAM provided there aren't any interactions (the second method works better for this).\n\nWe are going to use the ***insuraceData*** package, load up the ***AutoClaims*** dataset and then build a simple GLM for claim severity. We don't care that the model itself I just want to focus on the actual ratetabley bits.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(insuranceData)\nlibrary(data.table)\nlibrary(mgcv) # fit gam\n\ndata(AutoClaims)\n\n\n\n# fit numeric variables as GAMs to avoid having to\nclaims_glm <- mgcv::gam(PAID ~\n                    s(pmin(AGE,95),k=3, bs=\"cr\") +\n                    GENDER + \n                    CLASS +\n                    STATE,\n                  data = AutoClaims,\n                  family = Gamma(\"log\"))\n\n\n# model output\nsummary(claims_glm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFamily: Gamma \nLink function: log \n\nFormula:\nPAID ~ s(pmin(AGE, 95), k = 3, bs = \"cr\") + GENDER + CLASS + \n    STATE\n\nParametric coefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    7.372967   0.121985  60.441  < 2e-16 ***\nGENDERM       -0.008085   0.035549  -0.227  0.82009    \nCLASSC11      -0.001279   0.068666  -0.019  0.98513    \nCLASSC1A      -0.052953   0.169438  -0.313  0.75465    \nCLASSC1B       0.032155   0.087917   0.366  0.71457    \nCLASSC1C      -0.214266   0.235316  -0.911  0.36257    \nCLASSC2       -0.318220   0.188683  -1.687  0.09174 .  \nCLASSC6       -0.018006   0.078295  -0.230  0.81811    \nCLASSC7        0.002340   0.072101   0.032  0.97411    \nCLASSC71       0.016351   0.070098   0.233  0.81557    \nCLASSC72       0.225273   0.163104   1.381  0.16728    \nCLASSC7A       0.055853   0.143855   0.388  0.69783    \nCLASSC7B       0.166986   0.078150   2.137  0.03265 *  \nCLASSC7C       0.195217   0.166980   1.169  0.24240    \nCLASSF1       -0.078495   0.267407  -0.294  0.76912    \nCLASSF11       0.116224   0.230421   0.504  0.61400    \nCLASSF6       -0.006399   0.130963  -0.049  0.96103    \nCLASSF7       -0.495043   0.192008  -2.578  0.00995 ** \nCLASSF71      -0.059815   0.157060  -0.381  0.70333    \nSTATESTATE 02  0.108242   0.117960   0.918  0.35885    \nSTATESTATE 03  0.118306   0.133515   0.886  0.37560    \nSTATESTATE 04  0.059744   0.123815   0.483  0.62945    \nSTATESTATE 06  0.263534   0.124330   2.120  0.03407 *  \nSTATESTATE 07  0.181269   0.140526   1.290  0.19712    \nSTATESTATE 10  0.157797   0.139320   1.133  0.25741    \nSTATESTATE 11  0.097603   0.483004   0.202  0.83986    \nSTATESTATE 12  0.393129   0.143128   2.747  0.00604 ** \nSTATESTATE 13  0.225654   0.147946   1.525  0.12725    \nSTATESTATE 14  0.050829   0.154268   0.329  0.74180    \nSTATESTATE 15  0.083418   0.114447   0.729  0.46610    \nSTATESTATE 17  0.210776   0.128050   1.646  0.09980 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                   edf Ref.df     F p-value  \ns(pmin(AGE, 95)) 1.923  1.994 3.757  0.0197 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.0036   Deviance explained = 1.46%\nGCV = 1.1323  Scale est. = 1.9841    n = 6773\n```\n:::\n:::\n\n\nNow we build a function that can take in the model, the dataset, the variable we want to build the relativity table for and a set of values we want to test it on using the rubber-ducky approach\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmake_rate_table <- function(model,dataset,variable, variable_values){\n \n  dataset <- setDT(dataset) # convert to data.table\n  base_prediction <- dataset[sample(.N, 1)] #sample a single line from the dataset\n  col_nr <- which(names(dataset)==variable) # find the column number for the variable\n  base_prediction$rep <- length(variable_values) #new column that tells us how many lines of data we need to duplicate our sample\n \n  expd <- splitstackshape::expandRows(base_prediction, \"rep\")\n  expd[,col_nr] <- variable_values # replace the variable with the values provided\n \n  # predictions <- predict.glm(model, newdata = expd, type= \"response\")\n  predictions <- mgcv::predict.gam(model, newdata = expd, type= \"response\")\n \n  # scale the predictions relative to the lowest value so we get relativities\n  return(data.frame(Level = variable_values,\n                    Relativity = predictions/min(predictions)))\n \n \n}\n```\n:::\n\n\nWe can apply this function to create the rate table for the Driver Age variable\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create the table of relativities\nage_relativities <- make_rate_table(claims_glm,\n                dataset = AutoClaims,\n                variable = \"AGE\",\n                variable_values = sort(unique(AutoClaims$AGE)))\n\n# lets look at the table\nknitr::kable(age_relativities[1:10,])\n```\n\n::: {.cell-output-display}\n| Level| Relativity|\n|-----:|----------:|\n|    50|   1.059501|\n|    51|   1.053348|\n|    52|   1.047287|\n|    53|   1.041370|\n|    54|   1.035650|\n|    55|   1.030180|\n|    56|   1.025009|\n|    57|   1.020187|\n|    58|   1.015762|\n|    59|   1.011785|\n:::\n\n```{.r .cell-code}\n# plot these\nggplot(age_relativities,aes(x=Level,y=Relativity))+\n  geom_line()+\n  geom_point()+\n  ggtitle(\"Driver Age Relativities\",\n          subtitle = \"Very weird shape!\")+\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Ratetables_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nWe've created our first table for the driver age. We now need to do this for all the other variables using the same idea.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheme_set(theme_minimal())\n# List of variables\nvariables = c(\"GENDER\", \"CLASS\", \"STATE\")\n\n# Initialize an empty list to store the relativity tables\nrelativities = list()\n\n# Loop to create relativity tables\nfor (var in variables) {\n  relativities[[var]] <- make_rate_table(claims_glm,\n                                         dataset = AutoClaims,\n                                         variable = var,\n                                         variable_values = sort(unique(AutoClaims[[var]])))\n \n\n}\n\n\n# Initialize an empty list for plots\nplots = list()\n\n# Loop to create ggplot objects for each relativity\nfor (var in names(relativities)) {\n  plots[[var]] <- ggplot(relativities[[var]], aes(x=Level, y=Relativity)) +\n                    geom_col() +\n                    theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=1)) + \n                    ggtitle(paste(var, \"Relativities\"))\n}\n\n# Loop to display plots\nfor (plot_name in names(plots)) {\n  print(plots[[plot_name]])\n}\n```\n\n::: {.cell-output-display}\n![](Ratetables_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Ratetables_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Ratetables_files/figure-html/unnamed-chunk-8-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plotting (example for one plot, you can loop or selectively plot as needed)\n```\n:::\n\n\n### Rebasing\n\nOk now you'll see the first downside to this approach is we need to adjust the intercept of the model to make the predictions tally up to the original GLM. To make predictions with this model we will left-join the original dataset to the relativities and calculate the new intercept required to make the predictions line up.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# we need to join the main dataset to the relativity tables\nTally <- AutoClaims |>\n  left_join(relativities[['GENDER']] |>\n              select(Level, Relativity_1 = Relativity),\n            by = c(\"GENDER\" = \"Level\")) |>\n  left_join(relativities[['CLASS']] |> select(Level, Relativity_2 = Relativity),\n            by = c(\"CLASS\" = \"Level\")) |>\n  left_join(relativities[['STATE']] |> select(Level, Relativity_3 = Relativity),\n            by = c(\"STATE\" = \"Level\")) |>\n  left_join(age_relativities |> select(Level, Relativity_4 = Relativity),\n            by = c(\"AGE\" = \"Level\")) |>\n  mutate(AllRelativities = matrixStats::rowProds(as.matrix(across(\n    matches(\"Relativity_[0-9]\")\n  )))) |>\n  mutate(BaseRate = mean(AutoClaims$PAID)) |> # set the baserate = avg claim size as starting point\n  mutate(RateTablePredictions = AllRelativities * BaseRate) |>\n  mutate(GLM_Prediction = predict(claims_glm, AutoClaims, type = \"response\"))\n\n\nhead(Tally)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      STATE CLASS GENDER AGE    PAID Relativity_1 Relativity_2 Relativity_3\n1: STATE 14   C6       M  97 1134.44     1.000000     1.611293     1.052143\n2: STATE 15   C6       M  96 3761.24     1.000000     1.611293     1.086996\n3: STATE 15   C11      M  95 7842.31     1.000000     1.638471     1.086996\n4: STATE 15   F6       F  95 2384.67     1.008118     1.630104     1.086996\n5: STATE 15   F6       M  95  650.00     1.000000     1.630104     1.086996\n6: STATE 15   F6       M  95  391.12     1.000000     1.630104     1.086996\n   Relativity_4 AllRelativities BaseRate RateTablePredictions GLM_Prediction\n1:      1.48686        2.520691 1853.035             4670.927       2322.296\n2:      1.48686        2.604190 1853.035             4825.654       2399.223\n3:      1.48686        2.648116 1853.035             4907.050       2439.691\n4:      1.48686        2.655980 1853.035             4921.622       2446.937\n5:      1.48686        2.634592 1853.035             4881.990       2427.232\n6:      1.48686        2.634592 1853.035             4881.990       2427.232\n```\n:::\n:::\n\n\nThankfully its quite easy to this as all we need to do is adjust the intercept/baserate of the model such that it still gives the same average response as the original GLM. Lets show what this looks like when plotting the actual vs expected values for the AGE variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# lets plot the predictions against the GLM versus the AGE variable\n Tally |>\n   group_by(AGE) |>\n   summarise(\n     Actual = mean(PAID),\n            GLM = mean(GLM_Prediction),\n            RateTable = mean(RateTablePredictions),\n            RateTable_Rebased = mean(RateTablePredictions*(mean(Tally$GLM)/(mean(Tally$RateTablePredictions))))) |>\n   ggplot()+\n   geom_line(aes(x=AGE,y=Actual,group=1,colour=\"Actual\"))+\n   geom_line(aes(x=AGE,y=GLM,group=1,colour=\"GLM\"))+\n   geom_line(aes(x=AGE,y=RateTable,group=1,colour=\"RateTable\"))+\n   geom_point(aes(x=AGE,y=RateTable_Rebased,group=1,colour=\"RateTable_Rebased\"),size=2)+\n   ggtitle(\"Actual vs Predicted Comparison for AGE\", subtitle = \"Rebasing makes it align perfectly with the original GLM\")\n```\n\n::: {.cell-output-display}\n![](Ratetables_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nAs you can see the predictions from the ratetables line up exactly with the original model once we rebase the predictions. However while rebasing is not difficult it is tedious, especially when you have dozens of variables to work through.\n\n### Interactions\n\nThe second thing to be aware of with this approach is it can give misleading relativities when you're trying to isolate the effect of interactions. This is because ***ICE*** will vary all aspects of the model that contain the variable of interest **including the** **interactions**, so for example say we had *AGE* interacted with *STATE,* when varying the *AGE* variable to create its ratetable, we will also be picking up the effect of *AGE*:*STATE*. This is a pain to work reverse out and my suggestion when using ***ICE*** is to make duplicates for all variables you want to interact in order to isolate their effects.\n\nAgain this is best shown with an example. Lets interact *AGE* and *STATE* and illustrate the differences in relativities caused by interactions when blindly applying the ***ICE*** approach:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cowplot)\n\n# new model interacting AGE & CLASS\nclaims_glm_interactions <- mgcv::gam(PAID ~\n                    s(pmin(AGE,95),k=3, bs=\"cr\") +\n                    CLASS +\n                    STATE +\n                    GENDER + \n                    AGE:STATE,\n                  data = AutoClaims,\n                  family = Gamma(\"log\"))\n\n# duplicate AGE and CLASS and build new GLM off the duplicated columns\nAutoClaims$AGE_2 <- AutoClaims$AGE\nAutoClaims$STATE_2 <- AutoClaims$STATE\n\n\nclaims_glm_interactions2 <- mgcv::gam(PAID ~\n                    s(pmin(AGE,95),k=3, bs=\"cr\") +\n                    CLASS +\n                    STATE +\n                    GENDER + \n                    AGE_2:STATE_2,\n                  data = AutoClaims,\n                  family = Gamma(\"log\"))\n\n\n# ICE function for two variables\nmake_rate_table_2var <- function(model,\n                                 dataset,\n                                 variable1,\n                                 variable2, \n                                 variable1_values,\n                                 variable2_values\n                                 ){\n \n  dataset <- setDT(dataset) # convert to data.table\n  base_prediction <- dataset[sample(.N, 1)] #sample a single line from the dataset\n  \n  baseline <- as.numeric(mgcv::predict.gam(model, newdata = base_prediction, type= \"response\"))\n  \n  \n  col_nr1 <- which(names(dataset)==variable1) # find the column number for the 1st variable\n  col_nr2 <- which(names(dataset)==variable2) # find the column number for the 2nd variable\n  \n  # create every unique combination of the var1 and var2\n  uniq_comb <- expand.grid(variable1_values,variable2_values)\n  colnames(uniq_comb) <- c(variable1,variable2)\n  \n  base_prediction$rep <- nrow(uniq_comb) #new column that tells us how many lines of data we need to duplicate our sample\n \n  expd <- splitstackshape::expandRows(base_prediction, \"rep\")\n  expd[,col_nr1] <- uniq_comb[,1] # replace the variable with the values provided\n  expd[,col_nr2] <- uniq_comb[,2] # replace the variable with the values provided\n \n  # predictions <- predict.glm(model, newdata = expd, type= \"response\")\n  predictions <- mgcv::predict.gam(model, newdata = expd, type= \"response\")\n \n  expd <- data.frame(expd)\n  \n  final_frame = data.frame(expd[,col_nr1],\n                    expd[,col_nr2],\n                    Relativity = predictions/(baseline))\n  \n  colnames(final_frame) <- c(variable1,variable2,\"Relativity\")\n  \n  # scale the predictions relative to the lowest value so we get relativities\n  return(final_frame)\n \n \n}\n \n\n\np1 <- make_rate_table_2var(claims_glm_interactions,\n                                 AutoClaims,\n                                 \"AGE\",\n                                 \"STATE\", \n                                variable1_values =  sort(unique(AutoClaims$AGE)),\n                                variable2_values =  sort(unique(AutoClaims$STATE))\n                                 ) %>% \n  ggplot(aes(x=AGE,y=Relativity,group=STATE, colour=STATE))+\n  geom_line()+\n  ggtitle(\"AGE:STATE Incorrect\", subtitle = \"Lies, deception\")\n\np2 <- make_rate_table_2var(claims_glm_interactions2,\n                                 AutoClaims,\n                                 \"AGE_2\",\n                                 \"STATE_2\", \n                                variable1_values =  sort(unique(AutoClaims$AGE_2)),\n                                variable2_values =  sort(unique(AutoClaims$STATE_2))\n                                 ) %>% \n  ggplot(aes(x=AGE_2,y=Relativity,group=STATE_2, colour=STATE_2))+\n  geom_line()+\n  ggtitle(\"AGE:STATE Correct\", subtitle = \"The other thing\")\n\n\nplot_grid(p1, p2)\n```\n\n::: {.cell-output-display}\n![](Ratetables_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nAs you can see above, the two graphs show totally different relativities for what should be the same thing, however this is simply because in the first approach we are picking up the effect of the interactions and the individual effects of *AGE* and *STATE.* The second graph is the true interaction effect and is what would be used in your ratetable.\n\nThere is another method I use to build ratetables to try and get around the rebasing and interaction issues which I will write about in future. This second method is a lot more 'fiddly' and needs adjusting to different modelling packages but does involve the use of the word *matrix* which immediately gives us extra LinkedIn-points.\n",
    "supporting": [
      "Ratetables_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}